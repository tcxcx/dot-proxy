import { Logger } from '@subsquid/logger';
import { OldSpecsBundle, OldTypesBundle } from '@subsquid/substrate-runtime/lib/metadata';
import { PolkadotjsTypesBundle } from '@subsquid/substrate-runtime/lib/metadata/old/typesBundle-polkadotjs';
import { Database } from '@subsquid/util-internal-processor-tools';
import { Range } from '@subsquid/util-internal-range';
import { Chain } from './chain';
import { Block, FieldSelection } from './interfaces/data';
import { CallRequest, ContractsContractEmittedRequest, EthereumTransactRequest, EventRequest, EvmLogRequest, GearMessageQueuedRequest, GearUserMessageSentRequest, ReviveContractEmittedRequest } from './interfaces/data-request';
export interface RpcEndpointSettings {
    /**
     * RPC endpoint URL (either http(s) or ws(s))
     */
    url: string;
    /**
     * Maximum number of ongoing concurrent requests
     */
    capacity?: number;
    /**
     * Maximum number of requests per second
     */
    rateLimit?: number;
    /**
     * Request timeout in `ms`
     */
    requestTimeout?: number;
    /**
     * Maximum number of requests in a single batch call
     */
    maxBatchCallSize?: number;
    /**
     * HTTP headers
     */
    headers?: Record<string, string>;
}
export interface RpcDataIngestionSettings {
    /**
     * Poll interval for new blocks in `ms`
     *
     * Poll mechanism is used to get new blocks via HTTP connection.
     */
    headPollInterval?: number;
    /**
     * When websocket subscription is used to get new blocks,
     * this setting specifies timeout in `ms` after which connection
     * will be reset and subscription re-initiated if no new block where received.
     */
    newHeadTimeout?: number;
    /**
     * Disable RPC data ingestion entirely
     */
    disabled?: boolean;
}
export interface GatewaySettings {
    /**
     * Subsquid Network Gateway url
     */
    url: string;
    /**
     * Request timeout in ms
     */
    requestTimeout?: number;
}
/**
 * @deprecated
 */
export type ArchiveSettings = GatewaySettings;
/**
 * @deprecated
 */
export interface DataSource {
    /**
     * Subsquid archive endpoint URL
     */
    archive?: string;
    /**
     * Chain node RPC endpoint URL
     */
    chain: string | RpcEndpointSettings;
}
interface BlockRange {
    range?: Range;
}
/**
 * API and data that is passed to the data handler
 */
export interface DataHandlerContext<Store, Fields extends FieldSelection> {
    /**
     * @internal
     */
    _chain: Chain;
    /**
     * An instance of a structured logger.
     */
    log: Logger;
    /**
     * Storage interface provided by the database
     */
    store: Store;
    /**
     * List of blocks to map and process
     */
    blocks: Block<Fields>[];
    /**
     * Signals, that the processor reached the head of a chain.
     *
     * The head block is always included in `.blocks`.
     */
    isHead: boolean;
}
export type SubstrateBatchProcessorFields<T> = T extends SubstrateBatchProcessor<infer F> ? F : never;
/**
 * Provides methods to configure and launch data processing.
 */
export declare class SubstrateBatchProcessor<F extends FieldSelection = {}> {
    private requests;
    private fields?;
    private blockRange?;
    private finalityConfirmation?;
    private archive?;
    private rpcEndpoint?;
    private rpcIngestSettings?;
    private typesBundle?;
    private prometheus;
    private running;
    /**
     * @deprecated Use {@link .setGateway()}
     */
    setArchive(url: string | GatewaySettings): this;
    /**
     * Set Subsquid Network Gateway endpoint (ex Archive).
     *
     * Subsquid Network allows to get data from finalized blocks up to
     * infinite times faster and more efficient than via regular RPC.
     *
     * @example
     * processor.setGateway('https://v2.archive.subsquid.io/network/kusama')
     */
    setGateway(url: string | GatewaySettings): this;
    /**
     * Set chain RPC endpoint
     *
     * @example
     * // just pass a URL
     * processor.setRpcEndpoint('https://kusama-rpc.polkadot.io')
     *
     * // adjust some connection options
     * processor.setRpcEndpoint({
     *     url: 'https://kusama-rpc.polkadot.io',
     *     rateLimit: 10
     * })
     */
    setRpcEndpoint(url: string | RpcEndpointSettings): this;
    /**
     * Sets blockchain data source.
     *
     * @example
     * processor.setDataSource({
     *     archive: 'https://v2.archive.subsquid.io/network/kusama',
     *     chain: 'https://kusama-rpc.polkadot.io'
     * })
     *
     * @deprecated Use separate {@link .setGateway()} and {@link .setRpcEndpoint()} methods
     * to specify data sources.
     */
    setDataSource(src: DataSource): this;
    /**
     * Set up RPC data ingestion settings
     */
    setRpcDataIngestionSettings(settings: RpcDataIngestionSettings): this;
    /**
     * @deprecated Use {@link .setRpcDataIngestionSettings()} instead
     */
    setChainPollInterval(ms: number): this;
    /**
     * Never use RPC endpoint for data ingestion.
     *
     * @deprecated This is the same as `.setRpcDataIngestionSettings({disabled: true})`
     */
    useArchiveOnly(yes?: boolean): this;
    /**
     * Limits the range of blocks to be processed.
     *
     * When the upper bound is specified,
     * the processor will terminate with exit code 0 once it reaches it.
     */
    setBlockRange(range?: Range): this;
    /**
     * Distance from the head block behind which all blocks are considered to be finalized.
     *
     * By default, the processor will track finalized blocks via `chain_getFinalizedHead`.
     * Configure it only if `chain_getFinalizedHead` doesnâ€™t return the expected info.
     */
    setFinalityConfirmation(nBlocks: number): this;
    /**
     * Configure a set of fetched fields
     */
    setFields<T extends FieldSelection>(fields: T): SubstrateBatchProcessor<T>;
    private add;
    /**
     * By default, the processor will fetch only blocks
     * which contain requested items. This method
     * modifies such behaviour to fetch all chain blocks.
     *
     * Optionally a range of blocks can be specified
     * for which the setting should be effective.
     */
    includeAllBlocks(range?: Range): this;
    addEvent(options: EventRequest & BlockRange): this;
    addCall(options: CallRequest & BlockRange): this;
    addEvmLog(options: EvmLogRequest & BlockRange): this;
    addEthereumTransaction(options: EthereumTransactRequest & BlockRange): this;
    addContractsContractEmitted(options: ContractsContractEmittedRequest & BlockRange): this;
    addGearMessageQueued(options: GearMessageQueuedRequest & BlockRange): this;
    addGearUserMessageSent(options: GearUserMessageSentRequest & BlockRange): this;
    addReviveContractEmitted(options: ReviveContractEmittedRequest & BlockRange): this;
    /**
     * Sets types bundle.
     *
     * Types bundle is only required for blocks which have
     * metadata version below 14 and only if we don't have built-in
     * support for the chain in question.
     *
     * Subsquid project has its own types bundle format,
     * however, most of polkadotjs types bundles will work as well.
     *
     * Types bundle can be specified in 2 different ways:
     *
     * 1. as a name of a JSON file
     * 2. as an {@link OldTypesBundle} or {@link OldSpecsBundle} or {@link PolkadotjsTypesBundle} object
     *
     * @example
     * // A path to a JSON file resolved relative to `cwd`.
     * processor.setTypesBundle('typesBundle.json')
     *
     * // OldTypesBundle object
     * processor.setTypesBundle({
     *     types: {
     *         Foo: 'u8'
     *     }
     * })
     */
    setTypesBundle(bundle: string | OldTypesBundle | OldSpecsBundle | PolkadotjsTypesBundle): this;
    /**
     * Sets the port for a built-in prometheus metrics server.
     *
     * By default, the value of `PROMETHEUS_PORT` environment
     * variable is used. When it is not set,
     * the processor will pick up an ephemeral port.
     */
    setPrometheusPort(port: number | string): this;
    private assertNotRunning;
    private getSquidId;
    private getChainRpcClient;
    private getRpcDataSource;
    private getArchiveDataSource;
    private getLogger;
    private getBatchRequests;
    private getChain;
    private processBatch;
    /**
     * Run data processing.
     *
     * This method assumes full control over the current OS process as
     * it terminates the entire program in case of error or
     * at the end of data processing.
     *
     * @param database - database is responsible for providing storage to the data handler
     * and persisting mapping progress and status.
     *
     * @param handler - The data handler, see {@link DataHandlerContext} for an API available to the handler.
     */
    run<Store>(database: Database<Store>, handler: (ctx: DataHandlerContext<Store, F>) => Promise<void>): void;
}
export {};
//# sourceMappingURL=processor.d.ts.map